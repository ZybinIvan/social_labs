{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №2 и №3\n",
    "## Кластеризация текстовых данных и изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1: Кластеризация текстовых данных"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T04:12:19.605742Z",
     "start_time": "2025-11-13T04:12:19.535178Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Загрузка текстовых данных\n",
    "text_messages_clean = []\n",
    "\n",
    "with open(r\"posts.json\") as file:\n",
    "    for line in file.readlines():\n",
    "        wall = json.loads(line)\n",
    "        parsed_response = json.JSONDecoder().decode(json.dumps(wall))\n",
    "        nodes = []\n",
    "        for key, value in parsed_response.items():\n",
    "            nodes.append(value.get('items'))\n",
    "        for node in nodes:\n",
    "            for post in node:\n",
    "                text = post.get(\"text\")\n",
    "                if text is not None:\n",
    "                    text_messages_clean.append(text)\n",
    "\n",
    "print(f\"Загружено сообщений: {len(text_messages_clean)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено сообщений: 486\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T04:12:25.672132Z",
     "start_time": "2025-11-13T04:12:25.660845Z"
    }
   },
   "source": [
    "# Подсчет слов\n",
    "words = {}\n",
    "\n",
    "for message in text_messages_clean:\n",
    "    if message is None:\n",
    "        continue\n",
    "    for word in message.split():\n",
    "        if word not in words:\n",
    "            words[word] = 1\n",
    "        else:\n",
    "            words[word] += 1\n",
    "\n",
    "words = dict(sorted(words.items(), key=lambda item: item[1], reverse=True))\n",
    "print(f\"Уникальных слов: {len(words)}\")\n",
    "print(\"Топ-20 слов:\")\n",
    "for i, (word, count) in enumerate(list(words.items())[:20]):\n",
    "    print(f\"{i+1}. {word}: {count}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальных слов: 7280\n",
      "Топ-20 слов:\n",
      "1. в: 390\n",
      "2. и: 376\n",
      "3. на: 213\n",
      "4. с: 141\n",
      "5. не: 139\n",
      "6. —: 118\n",
      "7. что: 85\n",
      "8. -: 82\n",
      "9. по: 80\n",
      "10. В: 71\n",
      "11. для: 55\n",
      "12. это: 52\n",
      "13. за: 52\n",
      "14. из: 44\n",
      "15. или: 44\n",
      "16. а: 42\n",
      "17. до: 41\n",
      "18. к: 41\n",
      "19. как: 40\n",
      "20. –: 32\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T04:12:32.522726Z",
     "start_time": "2025-11-13T04:12:28.749699Z"
    }
   },
   "source": [
    "# Импорт библиотек для обработки текста\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Загрузка ресурсов NLTK\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T04:12:35.780701Z",
     "start_time": "2025-11-13T04:12:35.776668Z"
    }
   },
   "source": [
    "# Функции токенизации и стемминга\n",
    "def token_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) \n",
    "              for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[а-яА-Я]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    \n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def token_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) \n",
    "              for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[а-яА-Я]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    \n",
    "    return filtered_tokens"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T04:12:38.437027Z",
     "start_time": "2025-11-13T04:12:38.089618Z"
    }
   },
   "source": [
    "# TF-IDF векторизация\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('russian')\n",
    "stopwords.extend(['что', 'это', 'так', 'вот', 'быть', 'как',\n",
    "                  'в', 'к', 'на', 'а', 'от', 'о', \n",
    "                  'чут', 'даж', 'еще'])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.75,\n",
    "    max_features=10000,\n",
    "    min_df=0.01,\n",
    "    stop_words=stopwords,\n",
    "    use_idf=True,\n",
    "    tokenizer=token_and_stem,\n",
    "    ngram_range=(1,3)\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_messages_clean)\n",
    "\n",
    "print(f\"Размерность TF-IDF матрицы: {tfidf_matrix.shape}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\projects\\university\\social\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "G:\\projects\\university\\social\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['бол', 'больш', 'будт', 'быт', 'вед', 'впроч', 'всег', 'всегд', 'друг', 'е', 'ег', 'ем', 'есл', 'ест', 'ещ', 'зач', 'зде', 'ил', 'иногд', 'когд', 'конечн', 'куд', 'лучш', 'межд', 'мен', 'мног', 'мо', 'можн', 'нег', 'нельз', 'нибуд', 'никогд', 'нич', 'опя', 'посл', 'пот', 'почт', 'разв', 'сво', 'себ', 'совс', 'теб', 'тепер', 'тог', 'тогд', 'тож', 'тольк', 'хорош', 'хот', 'чег', 'эт'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность TF-IDF матрицы: (486, 321)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применение методов кластеризации\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering, DBSCAN\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters_km = km.labels_.tolist()\n",
    "\n",
    "mbk = MiniBatchKMeans(init='random', n_clusters=num_clusters, random_state=42)\n",
    "mbk.fit(tfidf_matrix)\n",
    "clusters_mbk = mbk.labels_.tolist()\n",
    "\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(tfidf_matrix)\n",
    "clusters_dbscan = db.labels_\n",
    "\n",
    "agglo = AgglomerativeClustering(n_clusters=num_clusters, metric='euclidean')\n",
    "clusters_agglo = agglo.fit_predict(tfidf_matrix.toarray())\n",
    "\n",
    "print(f\"K-means: {pd.Series(clusters_km).value_counts().sort_index().to_dict()}\")\n",
    "print(f\"MiniBatchKMeans: {pd.Series(clusters_mbk).value_counts().sort_index().to_dict()}\")\n",
    "print(f\"DBSCAN: {pd.Series(clusters_dbscan).value_counts().sort_index().to_dict()}\")\n",
    "print(f\"Аггломеративная: {pd.Series(clusters_agglo).value_counts().sort_index().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ косинусных расстояний\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(dist, cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('Косинусные расстояния между документами')\n",
    "plt.xlabel('Документ')\n",
    "plt.ylabel('Документ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 2D визуализация\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "icpa_2d = IncrementalPCA(n_components=2, batch_size=16)\n",
    "icpa_2d.fit(dist)\n",
    "demo2d = icpa_2d.transform(dist)\n",
    "xs_2d, ys_2d = demo2d[:, 0], demo2d[:, 1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(xs_2d, ys_2d, marker='.', s=5, alpha=0.6)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('2D визуализация PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 3D визуализация\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "icpa_3d = IncrementalPCA(n_components=3, batch_size=16)\n",
    "icpa_3d.fit(dist)\n",
    "demo3d = icpa_3d.transform(dist)\n",
    "xs_3d, ys_3d, zs_3d = demo3d[:, 0], demo3d[:, 1], demo3d[:, 2]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(xs_3d, ys_3d, zs_3d, marker='.', s=5, alpha=0.6)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_title('3D визуализация PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2: Кластеризация изображений"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T04:13:44.991479Z",
     "start_time": "2025-11-13T04:13:44.986233Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "# Импорт библиотек для работы с API ВКонтакте и загрузки изображений\n",
    "from urllib.request import urlretrieve\n",
    "import vk\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Токен доступа (необходимо заполнить)\n",
    "load_dotenv()\n",
    "secret_token = os.getenv(\"VK_ACCESS_TOKEN\") # Измените на ваш токен\n",
    "# Авторизация\n",
    "try:\n",
    "    vkapi = vk.API(access_token=secret_token, v=\"5.81\")\n",
    "    print(\"Авторизация выполнена успешно\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка авторизации: {e}\")\n",
    "    print(\"Используйте действительный токен доступа\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Авторизация выполнена успешно\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T04:13:52.064386Z",
     "start_time": "2025-11-13T04:13:51.658790Z"
    }
   },
   "source": [
    "# Извлечение ID альбома и владельца из URL\n",
    "url = \"https://vk.com/album-97980111_248419714\"\n",
    "\n",
    "album_id = url.split('/')[-1].split('_')[1]\n",
    "owner_id = url.split('/')[-1].split('_')[0].replace('album', '')\n",
    "\n",
    "print(f\"ID альбома: {album_id}\")\n",
    "print(f\"ID владельца: {owner_id}\")\n",
    "\n",
    "# Получение информации об альбоме\n",
    "try:\n",
    "    photos_count = vkapi.photos.getAlbums(owner_id=owner_id, album_ids=album_id)['items'][0]['size']\n",
    "    print(f\"Всего фотографий в альбоме: {photos_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при получении информации об альбоме: {e}\")\n",
    "    photos_count = 0"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID альбома: 248419714\n",
      "ID владельца: -97980111\n",
      "Всего фотографий в альбоме: 1679\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание директорий для сохранения фотографий\n",
    "if not os.path.exists('saved'):\n",
    "    os.mkdir('saved')\n",
    "\n",
    "photo_folder = f'saved/album{owner_id}_{album_id}'\n",
    "\n",
    "if not os.path.exists(photo_folder):\n",
    "    os.mkdir(photo_folder)\n",
    "\n",
    "print(f\"Директория для сохранения: {photo_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка фотографий из альбома (пропущена в целях демонстрации)\n",
    "# Раскомментируйте код ниже если хотите загрузить реальные фотографии\n",
    "\n",
    "# counter = 0\n",
    "# prog = 0\n",
    "# batch_size = 100\n",
    "# time_now = time.time()\n",
    "#\n",
    "# for j in range(math.ceil(photos_count / batch_size)):\n",
    "#     try:\n",
    "#         photos = vkapi.photos.get(owner_id=owner_id, album_id=album_id,\n",
    "#                                    count=batch_size, offset=j*batch_size, v=5.95)['items']\n",
    "#     except:\n",
    "#         continue\n",
    "#     for photo in photos:\n",
    "#         counter += 1\n",
    "#         sizes = photo['sizes']\n",
    "#         s = photo['sizes'][0]\n",
    "#         value_x, value_y = 0, 0\n",
    "#         for size in sizes:\n",
    "#             if value_x < size['width']:\n",
    "#                 value_x = size['width']\n",
    "#                 s = size\n",
    "#         url_ = s['url']\n",
    "#         try:\n",
    "#             filename = os.path.split(url_)[1].split('?')[0]\n",
    "#             urlretrieve(url_, os.path.join(photo_folder, filename))\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "print(\"Загрузка фотографий требует действительный токен доступа\")\n",
    "print(f\"Фотографии сохраняются в: {photo_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт библиотек для обработки изображений\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Класс для выделения доминирующих цветов\n",
    "class DominantColors:\n",
    "    def __init__(self, image_path, clusters):\n",
    "        self.CLUSTERS = clusters\n",
    "        self.IMAGE_PATH = image_path\n",
    "        self.IMAGE = None\n",
    "        self.COLORS = None\n",
    "        self.LABELS = None\n",
    "    \n",
    "    def dominantColors(self):\n",
    "        \"\"\"Выделение доминирующих цветов при помощи K-means\"\"\"\n",
    "        img = cv2.imread(self.IMAGE_PATH)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Не удалось загрузить изображение: {self.IMAGE_PATH}\")\n",
    "        \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.reshape((img.shape[0] * img.shape[1], 3))\n",
    "        self.IMAGE = img\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=self.CLUSTERS, random_state=42)\n",
    "        kmeans.fit(img)\n",
    "        \n",
    "        self.COLORS = kmeans.cluster_centers_\n",
    "        self.LABELS = kmeans.labels_\n",
    "        \n",
    "        return self.COLORS.astype(int)\n",
    "    \n",
    "    def plotHistogram(self):\n",
    "        \"\"\"Отрисовка гистограммы доминирующих цветов\"\"\"\n",
    "        numLabels = np.arange(0, self.CLUSTERS + 1)\n",
    "        (hist, _) = np.histogram(self.LABELS, bins=numLabels)\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= hist.sum()\n",
    "        \n",
    "        colors = self.COLORS[(-hist).argsort()]\n",
    "        hist = hist[(-hist).argsort()]\n",
    "        \n",
    "        chart = np.zeros((50, 500, 3), np.uint8)\n",
    "        start = 0\n",
    "        \n",
    "        for i in range(self.CLUSTERS):\n",
    "            end = start + hist[i] * 500\n",
    "            r, g, b = int(colors[i][0]), int(colors[i][1]), int(colors[i][2])\n",
    "            cv2.rectangle(chart, (int(start), 0), (int(end), 50), (b, g, r), -1)\n",
    "            start = end\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, 4))\n",
    "        pic_box = fig.add_subplot(121)\n",
    "        hist_box = fig.add_subplot(122)\n",
    "        \n",
    "        pic_box.axis(\"off\")\n",
    "        pic_box.set_title(\"Исходное изображение\")\n",
    "        try:\n",
    "            pic_box.imshow(plt.imread(self.IMAGE_PATH))\n",
    "        except:\n",
    "            pic_box.text(0.5, 0.5, \"Изображение не найдено\", ha='center')\n",
    "        \n",
    "        hist_box.axis(\"off\")\n",
    "        hist_box.set_title(\"Доминирующие цвета\")\n",
    "        hist_box.imshow(chart)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "print(\"Класс DominantColors создан успешно\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования класса DominantColors\n",
    "# Замените путь на путь к вашему изображению\n",
    "\n",
    "# img_path = 'saved/album-97980111_248419714/image.jpg'\n",
    "# clusters = 5\n",
    "# dc = DominantColors(img_path, clusters)\n",
    "# colors = dc.dominantColors()\n",
    "# print(f\"Доминирующие цвета (RGB): {colors}\")\n",
    "# dc.plotHistogram()\n",
    "\n",
    "print(\"Для использования на реальных изображениях укажите путь к файлу\")\n",
    "print(\"Пример: dc = DominantColors('saved/album.../image.jpg', 5)\")\n",
    "print(\"Затем вызовите: dc.dominantColors() и dc.plotHistogram()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кластеризация всех изображений в папке\n",
    "# directory = photo_folder\n",
    "# filenames = []\n",
    "# data = []\n",
    "# files_total = len(os.listdir(directory))\n",
    "#\n",
    "# if files_total == 0:\n",
    "#     print(f\"В папке {directory} нет изображений\")\n",
    "# else:\n",
    "#     for filename in os.listdir(directory):\n",
    "#         if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "#             data_iter = []\n",
    "#             filenames.append(filename)\n",
    "#             img = os.path.join(directory, filename)\n",
    "#             try:\n",
    "#                 clusters = 3\n",
    "#                 dc = DominantColors(img, clusters)\n",
    "#                 colors = dc.dominantColors()\n",
    "#                 for i in colors:\n",
    "#                     for j in i:\n",
    "#                         data_iter.append(j)\n",
    "#                 data.append(data_iter)\n",
    "#                 print(f\"{len(filenames)}/{files_total}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Ошибка: {e}\")\n",
    "\n",
    "print(\"Кластеризация всех изображений пропущена\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA анализ данных изображений\n",
    "# from sklearn.decomposition import PCA\n",
    "#\n",
    "# if len(data) > 0:\n",
    "#     np_data = np.asarray(data, dtype=np.float32)\n",
    "#     pca = PCA(n_components=3)\n",
    "#     XPCAreduced = pca.fit_transform(np_data)\n",
    "#     print(f\"Объяснённая дисперсия: {pca.explained_variance_ratio_}\")\n",
    "#     print(f\"Сумма: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "#     xs, ys, zs = np_data[:, 0], np_data[:, 1], np_data[:, 2]\n",
    "#     fig = plt.figure(figsize=(10, 8))\n",
    "#     ax = fig.add_subplot(111, projection='3d')\n",
    "#     ax.scatter(xs, ys, zs, c='blue', marker='o', s=50)\n",
    "#     ax.set_xlabel('Компонента 1 (R)')\n",
    "#     ax.set_ylabel('Компонента 2 (G)')\n",
    "#     ax.set_zlabel('Компонента 3 (B)')\n",
    "#     ax.set_title('3D визуализация цветов')\n",
    "#     plt.show()\n",
    "# else:\n",
    "#     print(\"Нет данных для анализа PCA\")\n",
    "\n",
    "print(\"PCA анализ пропущен (требуются обработанные данные)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторные работы №2-5\n",
    "## Машинное обучение и анализ данных социальных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа №2: Кластеризация текстовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Загрузка текстовых данных\n",
    "text_messages_clean = []\n",
    "\n",
    "with open(r\"posts.json\") as file:\n",
    "    for line in file.readlines():\n",
    "        wall = json.loads(line)\n",
    "        parsed_response = json.JSONDecoder().decode(json.dumps(wall))\n",
    "        nodes = []\n",
    "        for key, value in parsed_response.items():\n",
    "            nodes.append(value.get('items'))\n",
    "        for node in nodes:\n",
    "            for post in node:\n",
    "                text = post.get(\"text\")\n",
    "                if text is not None:\n",
    "                    text_messages_clean.append(text)\n",
    "\n",
    "print(f\"Загружено сообщений: {len(text_messages_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подсчет слов\n",
    "words = {}\n",
    "for message in text_messages_clean:\n",
    "    if message is None:\n",
    "        continue\n",
    "    for word in message.split():\n",
    "        if word not in words:\n",
    "            words[word] = 1\n",
    "        else:\n",
    "            words[word] += 1\n",
    "\n",
    "words = dict(sorted(words.items(), key=lambda item: item[1], reverse=True))\n",
    "print(f\"Уникальных слов: {len(words)}\")\n",
    "print(\"Топ-10 слов:\", list(words.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF и кластеризация\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "def token_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = [token for token in tokens if re.search('[а-яА-Я]', token)]\n",
    "    return [stemmer.stem(t) for t in filtered_tokens]\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('russian')\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.75, max_features=10000, min_df=0.01,\n",
    "    stop_words=stopwords, use_idf=True, tokenizer=token_and_stem, ngram_range=(1,3)\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_messages_clean)\n",
    "print(f\"TF-IDF матрица: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Кластеризация\n",
    "km = KMeans(n_clusters=5, random_state=42)\n",
    "km.fit(tfidf_matrix)\n",
    "print(f\"K-means кластеры: {pd.Series(km.labels_).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа №3: Кластеризация изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс для работы с изображениями\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans as KMeansImage\n",
    "\n",
    "class DominantColors:\n",
    "    def __init__(self, image_path, clusters):\n",
    "        self.CLUSTERS = clusters\n",
    "        self.IMAGE_PATH = image_path\n",
    "        self.IMAGE = None\n",
    "        self.COLORS = None\n",
    "        self.LABELS = None\n",
    "    \n",
    "    def dominantColors(self):\n",
    "        try:\n",
    "            img = cv2.imread(self.IMAGE_PATH)\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Не удалось загрузить: {self.IMAGE_PATH}\")\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img.reshape((img.shape[0] * img.shape[1], 3))\n",
    "            self.IMAGE = img\n",
    "            \n",
    "            kmeans = KMeansImage(n_clusters=self.CLUSTERS, random_state=42, n_init=10)\n",
    "            kmeans.fit(img)\n",
    "            \n",
    "            self.COLORS = kmeans.cluster_centers_\n",
    "            self.LABELS = kmeans.labels_\n",
    "            return self.COLORS.astype(int)\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка: {e}\")\n",
    "            return None\n",
    "\n",
    "print(\"Класс DominantColors создан\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа №4: Визуализация графов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт библиотек для графов\n",
    "import vk_api\n",
    "import time\n",
    "import pickle\n",
    "import networkx as nx\n",
    "\n",
    "# Функция для получения друзей\n",
    "def get_groups_user(friends_list, vk_tools):\n",
    "    friends_list_out = {}\n",
    "    for friend in friends_list:\n",
    "        try:\n",
    "            friends_list_out[friend] = vk_tools.get_all('friends.get', 100, {'user_id': friend})\n",
    "        except Exception as e:\n",
    "            friends_list_out[friend] = []\n",
    "        time.sleep(1)\n",
    "    return friends_list_out\n",
    "\n",
    "print(\"Функции для работы с VK API загружены\")\n",
    "print(\"Примечание: Для использования требуется действительный токен\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для создания графа\n",
    "def make_graph(friends_out, friends_friends, user_id):\n",
    "    graph = nx.Graph()\n",
    "    graph.add_node(user_id, size=friends_out[user_id]['count'])\n",
    "    \n",
    "    for i in friends_out[user_id]['items']:\n",
    "        try:\n",
    "            if len(friends_friends[i]) == 0:\n",
    "                continue\n",
    "            graph.add_node(i, size=friends_friends[i]['count'])\n",
    "            intersection = set(friends_out[user_id]['items']).intersection(set(friends_friends[i]['items']))\n",
    "            graph.add_edge(user_id, i, weight=len(intersection))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for i in range(len(friends_out[user_id]['items'])):\n",
    "        id1 = friends_out[user_id]['items'][i]\n",
    "        for k in range(i + 1, len(friends_out[user_id]['items'])):\n",
    "            id2 = friends_out[user_id]['items'][k]\n",
    "            try:\n",
    "                if len(friends_friends[id1]) == 0 or len(friends_friends[id2]) == 0:\n",
    "                    continue\n",
    "                intersection = set(friends_friends[id1]['items']).intersection(set(friends_friends[id2]['items']))\n",
    "                if len(intersection) > 0:\n",
    "                    graph.add_edge(id1, id2, weight=len(intersection))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return graph\n",
    "\n",
    "print(\"Функция make_graph создана\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа №5: Наивный байесовский классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наивный байесовский классификатор\n",
    "# Требуется файл Mandrill.xlsx\n",
    "\n",
    "# Этапы реализации:\n",
    "# 1. Токенизация твитов (преобразование букв, удаление пунктуации)\n",
    "# 2. Создание листов AppTokens и OtherTokens\n",
    "# 3. Разделение твитов на токены с помощью формул НАЙТИ и ПСТР\n",
    "# 4. Создание сводных таблиц AppTokensProbability и OtherTokensProbability\n",
    "# 5. Расчет условных вероятностей каждого токена\n",
    "# 6. Применение аддитивного сглаживания (сглаживание Лапласа)\n",
    "# 7. Расчет натуральных логарифмов вероятностей\n",
    "# 8. Тестирование на тестовом наборе твитов\n",
    "# 9. Классификация твитов путем сравнения вероятностей\n",
    "\n",
    "print(\"Наивный байесовский классификатор\")\n",
    "print(\"=\"*50)\n",
    "print(\"Этапы реализации:\")\n",
    "print(\"1. Токенизация твитов\")\n",
    "print(\"2. Расчет условных вероятностей\")\n",
    "print(\"3. Применение сглаживания Лапласа\")\n",
    "print(\"4. Расчет логарифмов вероятностей\")\n",
    "print(\"5. Классификация новых твитов\")\n",
    "print(\"\\nИспользуемые функции Excel/Calc:\")\n",
    "print(\"- СТРОЧН: преобразование букв в строчные\")\n",
    "print(\"- ПОДСТАВИТЬ: замена символов\")\n",
    "print(\"- НАЙТИ: поиск позиции пробела\")\n",
    "print(\"- ПСТР: извлечение подстроки (токена)\")\n",
    "print(\"- ДЛСТР: длина строки\")\n",
    "print(\"- ЕСЛИОШИБКА: обработка ошибок\")\n",
    "print(\"- ВПР: поиск значений в таблице\")\n",
    "print(\"- Сводные таблицы: подсчет частот токенов\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример формул для Excel/Calc\n",
    "formulas = {\n",
    "    \"Преобразование в строчные\": \"=СТРОЧН(A2)\",\n",
    "    \"Замена пунктуации\": \"=ПОДСТАВИТЬ(B2;\\\".\\\"; \\\" \\\")\",\n",
    "    \"Поиск позиции пробела\": \"=ЕСЛИОШИБКА(НАЙТИ(\\\" \\\";A152;B2+1);ДЛСТР(A152)+1)\",\n",
    "    \"Извлечение токена\": \"=ЕСЛИОШИБКА(ПСТР(A2;B2+1;B152-B2-1);\\\".\\\") \",\n",
    "    \"Длина токена\": \"=ДЛСТР(C2)\",\n",
    "    \"Вероятность токена\": \"=C6/C$828\",\n",
    "    \"Натуральный логарифм\": \"=LN(D6)\",\n",
    "    \"Поиск вероятности по токену\": \"=ЕСЛИ(ДЛСТР(D2)<=3;0;ЕСЛИ(ЕНД(ВПР(D2;$AppTokensProbability.$A$5:$E$827;5;0));LN(1/$AppTokensProbability.$C$828);ВПР(D2;$AppTokensProbability.$A$5:$E$827;5;0)))\"\n",
    "}\n",
    "\n",
    "print(\"Основные формулы для классификатора:\")\n",
    "print(\"=\"*70)\n",
    "for name, formula in formulas.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  {formula}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эффективность классификатора\n",
    "print(\"\\nЭффективность наивного байесовского классификатора:\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"Преимущества:\")\n",
    "print(\"  ✓ Простая реализация и интерпретация\")\n",
    "print(\"  ✓ Эффективен при работе с текстовыми данными\")\n",
    "print(\"  ✓ Хорошие результаты на практике\")\n",
    "print(\"  ✓ Масштабируемость\")\n",
    "print()\n",
    "print(\"Ограничения:\")\n",
    "print(\"  ✗ Предположение об условной независимости признаков\")\n",
    "print(\"  ✗ Проблема с редкими словами (решается сглаживанием)\")\n",
    "print(\"  ✗ Неправильная оценка вероятностей в крайних случаях\")\n",
    "print()\n",
    "print(\"Применение сглаживания Лапласа:\")\n",
    "print(\"  P(token|class) = (count + 1) / (total_tokens + |V|)\")\n",
    "print(\"  где |V| - размер словаря\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}